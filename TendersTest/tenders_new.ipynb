{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f327dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from keybert._model import KeyBERT\n",
    "import nltk, re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4c767d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenders_info = pd.read_csv('./Data/tenders_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c4bb007",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenders_info['text'] = tenders_info['Description'] + '.' + tenders_info['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56e457cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33736797",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = stopwords.words('english')\n",
    "kw_model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0297a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_tags(text):\n",
    "    keywords = kw_model.extract_keywords(text,\n",
    "                                                  keyphrase_ngram_range=(1, 3),\n",
    "                                                  stop_words='english',\n",
    "                                                  use_mmr=True,\n",
    "                                                  diversity=0.2)\n",
    "    if len(keywords) == 0:\n",
    "            keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 3), stop_words='english')\n",
    "    return keywords if len(keywords) > 0 else [('[none_tag]', 1)]\n",
    "def __agg_tags(self, text) -> pd.DataFrame:\n",
    "    reg_rule = r'\\((.*?)\\)'\n",
    "    text = str(text)\n",
    "\n",
    "    # Extract and reformat term-weight set\n",
    "    \n",
    "    split_result = tmp_df[0].str.split(',', expand=True).rename(columns={0: 'key',\n",
    "                                                                             1: 'value'}).reset_index()\n",
    "\n",
    "    merge_df = tmp_df.merge(split_result, on='index').drop('index', axis=1).rename(columns={'level_0': 'items'})\n",
    "    del tmp_df, split_result\n",
    "\n",
    "    merge_df['key'] = merge_df['key'].map(self.__split_words)\n",
    "    mapping_df = merge_df.explode('key')[['items', 'key', 'value']]\n",
    "    mapping_df['key_orig'] = mapping_df['key']\n",
    "    # mapping_df['key'] = mapping_df['key'].map(nltk.PorterStemmer().stem)\n",
    "\n",
    "    # Compute weight according to each word and ordering\n",
    "    mapping_df['value'] = mapping_df['value'].astype(float)\n",
    "    sum_df = mapping_df.groupby(['items', 'key', 'key_orig'])['value'].sum().reset_index().sort_values(\n",
    "            ['items', 'value'], ascending=False)\n",
    "\n",
    "    # Sampling top key\n",
    "    key_df = sum_df.groupby(['items']).head(1)[['items', 'key']]\n",
    "    merge_df = sum_df.merge(key_df, on=['items', 'key'])\n",
    "    return merge_df\n",
    "def __convert_word_type(word):\n",
    "    token = nltk.word_tokenize(word.lower())\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    pos_tagged = nltk.pos_tag(token)\n",
    "    words = []\n",
    "    Noun = list(filter(\n",
    "        lambda x: x[0] not in STOP_WORDS and x[0] != '/' and (x[1].startswith('NN') or x[1].startswith('JJ')),\n",
    "            pos_tagged))\n",
    "    for word, pos in Noun:\n",
    "        if pos.startswith('NN'):\n",
    "            word = lemmatizer.lemmatize(word, pos='n')\n",
    "        elif pos.startswith('JJ'):\n",
    "            word = lemmatizer.lemmatize(word, pos='a')\n",
    "        words.append(word)\n",
    "    text = \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2d2b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __extract_process(index, row):\n",
    "    text = row['text']\n",
    "    if len(text) == 0: return []\n",
    "    result_list = __get_tags(text)\n",
    "    print(result_list)\n",
    "    return index+4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "008cd061",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>_id</th>\n",
       "      <th>Agency</th>\n",
       "      <th>Title</th>\n",
       "      <th>Publish Date</th>\n",
       "      <th>Close Date</th>\n",
       "      <th>Category</th>\n",
       "      <th>Category_sub</th>\n",
       "      <th>Description</th>\n",
       "      <th>Eligibility</th>\n",
       "      <th>Value</th>\n",
       "      <th>Location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6233feb7c546c803a643a750</td>\n",
       "      <td>Department of Agriculture, Water and the Envir...</td>\n",
       "      <td>Australia-China Agricultural Cooperation Agree...</td>\n",
       "      <td>2017-02-10</td>\n",
       "      <td>2017-03-10</td>\n",
       "      <td>Overseas Advocacy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Australia–China Agricultural Cooperation A...</td>\n",
       "      <td>Any individual, organisation or business with ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACT, NSW, VIC, SA, WA, QLD, NT, TAS</td>\n",
       "      <td>australia–china agricultural cooperation agree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6233febac546c803a643a751</td>\n",
       "      <td>Department of Foreign Affairs and Trade</td>\n",
       "      <td>Australia-ASEAN Council 2017 Grant Round</td>\n",
       "      <td>2017-02-06</td>\n",
       "      <td>2017-03-27</td>\n",
       "      <td>Public Diplomacy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Australia-ASEAN Council's (AAC’s) mission ...</td>\n",
       "      <td>To be eligible you must: be one of the followi...</td>\n",
       "      <td>1000000-5000000</td>\n",
       "      <td>ACT, NSW, VIC, SA, WA, QLD, NT, TAS, Overseas</td>\n",
       "      <td>australia-asean council aac ’ mission knowledg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6233febcc546c803a643a752</td>\n",
       "      <td>Department of Agriculture, Water and the Envir...</td>\n",
       "      <td>Package Assisting Small Exporters (PASE) Round 2</td>\n",
       "      <td>2017-02-24</td>\n",
       "      <td>2017-03-27</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>341000 - Trade and Tourism</td>\n",
       "      <td>In the 2014-15 budget, the government committe...</td>\n",
       "      <td>The following can apply for a grant: - Organis...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACT, NSW, VIC, SA, WA, QLD, NT, TAS</td>\n",
       "      <td>-budget government year package small exporter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6233febec546c803a643a753</td>\n",
       "      <td>Department of Foreign Affairs and Trade</td>\n",
       "      <td>Council on Australia Latin America Relations</td>\n",
       "      <td>2017-02-06</td>\n",
       "      <td>2017-03-28</td>\n",
       "      <td>Public Diplomacy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Council on Australia Latin America Relatio...</td>\n",
       "      <td>To be eligible you must: be one of the followi...</td>\n",
       "      <td>2000000-4000000</td>\n",
       "      <td>ACT, NSW, VIC, SA, WA, QLD, NT, TAS, Overseas</td>\n",
       "      <td>council australia latin america relation coala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6233fec0c546c803a643a754</td>\n",
       "      <td>Department of Health</td>\n",
       "      <td>National Aged Care Advocacy Program (NACAP) Fu...</td>\n",
       "      <td>2017-02-16</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>Aged Care</td>\n",
       "      <td>331001 - Advocacy</td>\n",
       "      <td>The National Aged Care Advocacy Program provid...</td>\n",
       "      <td>The following entity types meet the eligibilit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACT, NSW, VIC, SA, WA, QLD, NT, TAS</td>\n",
       "      <td>national care advocacy program free independen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>6233fec3c546c803a643a755</td>\n",
       "      <td>Department of Foreign Affairs and Trade</td>\n",
       "      <td>Australia-Korea Foundation 2017 Grant Round</td>\n",
       "      <td>2017-02-06</td>\n",
       "      <td>2017-04-03</td>\n",
       "      <td>Public Diplomacy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Australia-Korea Foundation 2017 seeks to s...</td>\n",
       "      <td>To be eligible you must: be one of the follow...</td>\n",
       "      <td>1500000-3000000</td>\n",
       "      <td>ACT, NSW, VIC, SA, WA, QLD, NT, TAS, Overseas</td>\n",
       "      <td>australia-korea foundation seek australia-kore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6233fec5c546c803a643a756</td>\n",
       "      <td>Department of Foreign Affairs and Trade</td>\n",
       "      <td>Council for Australian-Arab Relations</td>\n",
       "      <td>2017-02-06</td>\n",
       "      <td>2017-04-03</td>\n",
       "      <td>Public Diplomacy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Council for Australian-Arab Relations seek...</td>\n",
       "      <td>To be eligible you must: be one of the followi...</td>\n",
       "      <td>2000000-4000000</td>\n",
       "      <td>ACT, NSW, VIC, SA, WA, QLD, NT, TAS, Overseas</td>\n",
       "      <td>council australian-arab relation australian-ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>6233fec7c546c803a643a757</td>\n",
       "      <td>Department of Foreign Affairs and Trade</td>\n",
       "      <td>Australia-Japan Foundation 2017 Grant Round</td>\n",
       "      <td>2017-02-06</td>\n",
       "      <td>2017-04-06</td>\n",
       "      <td>Public Diplomacy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Australia-Japan Foundation seeks to advanc...</td>\n",
       "      <td>To be eligible you must: be one of the follow...</td>\n",
       "      <td>500000-5000000</td>\n",
       "      <td>ACT, NSW, VIC, SA, WA, QLD, NT, TAS, Overseas</td>\n",
       "      <td>australia-japan foundation australia ’ engagem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>6233fec9c546c803a643a758</td>\n",
       "      <td>Department of Foreign Affairs and Trade</td>\n",
       "      <td>Australian Cultural Diplomacy Grants Program 2...</td>\n",
       "      <td>2017-02-06</td>\n",
       "      <td>2017-04-12</td>\n",
       "      <td>Public Diplomacy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Australian Cultural Diplomacy Grants Progr...</td>\n",
       "      <td>To be eligible you must: be one of the followi...</td>\n",
       "      <td>500000-6000000</td>\n",
       "      <td>ACT, NSW, VIC, SA, WA, QLD, NT, TAS, Overseas</td>\n",
       "      <td>australian cultural diplomacy grant program ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>6233feccc546c803a643a759</td>\n",
       "      <td>Department of the Prime Minister and Cabinet</td>\n",
       "      <td>National Women's Alliances</td>\n",
       "      <td>2017-03-16</td>\n",
       "      <td>2017-04-17</td>\n",
       "      <td>Women</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The National Women’s Alliance Grant Opportunit...</td>\n",
       "      <td>We can only accept applications from:  existin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACT, NSW, VIC, SA, WA, QLD, NT, TAS</td>\n",
       "      <td>national woman alliance grant opportunity form...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                       _id  \\\n",
       "0      0  6233feb7c546c803a643a750   \n",
       "1      1  6233febac546c803a643a751   \n",
       "2      2  6233febcc546c803a643a752   \n",
       "3      3  6233febec546c803a643a753   \n",
       "4      4  6233fec0c546c803a643a754   \n",
       "5      5  6233fec3c546c803a643a755   \n",
       "6      6  6233fec5c546c803a643a756   \n",
       "7      7  6233fec7c546c803a643a757   \n",
       "8      8  6233fec9c546c803a643a758   \n",
       "9      9  6233feccc546c803a643a759   \n",
       "\n",
       "                                              Agency  \\\n",
       "0  Department of Agriculture, Water and the Envir...   \n",
       "1            Department of Foreign Affairs and Trade   \n",
       "2  Department of Agriculture, Water and the Envir...   \n",
       "3            Department of Foreign Affairs and Trade   \n",
       "4                               Department of Health   \n",
       "5            Department of Foreign Affairs and Trade   \n",
       "6            Department of Foreign Affairs and Trade   \n",
       "7            Department of Foreign Affairs and Trade   \n",
       "8            Department of Foreign Affairs and Trade   \n",
       "9       Department of the Prime Minister and Cabinet   \n",
       "\n",
       "                                               Title Publish Date  Close Date  \\\n",
       "0  Australia-China Agricultural Cooperation Agree...   2017-02-10  2017-03-10   \n",
       "1           Australia-ASEAN Council 2017 Grant Round   2017-02-06  2017-03-27   \n",
       "2   Package Assisting Small Exporters (PASE) Round 2   2017-02-24  2017-03-27   \n",
       "3       Council on Australia Latin America Relations   2017-02-06  2017-03-28   \n",
       "4  National Aged Care Advocacy Program (NACAP) Fu...   2017-02-16  2017-03-31   \n",
       "5        Australia-Korea Foundation 2017 Grant Round   2017-02-06  2017-04-03   \n",
       "6              Council for Australian-Arab Relations   2017-02-06  2017-04-03   \n",
       "7        Australia-Japan Foundation 2017 Grant Round   2017-02-06  2017-04-06   \n",
       "8  Australian Cultural Diplomacy Grants Program 2...   2017-02-06  2017-04-12   \n",
       "9                         National Women's Alliances   2017-03-16  2017-04-17   \n",
       "\n",
       "            Category                Category_sub  \\\n",
       "0  Overseas Advocacy                         NaN   \n",
       "1   Public Diplomacy                         NaN   \n",
       "2     Small Business  341000 - Trade and Tourism   \n",
       "3   Public Diplomacy                         NaN   \n",
       "4          Aged Care           331001 - Advocacy   \n",
       "5   Public Diplomacy                         NaN   \n",
       "6   Public Diplomacy                         NaN   \n",
       "7   Public Diplomacy                         NaN   \n",
       "8   Public Diplomacy                         NaN   \n",
       "9              Women                         NaN   \n",
       "\n",
       "                                         Description  \\\n",
       "0  The Australia–China Agricultural Cooperation A...   \n",
       "1  The Australia-ASEAN Council's (AAC’s) mission ...   \n",
       "2  In the 2014-15 budget, the government committe...   \n",
       "3  The Council on Australia Latin America Relatio...   \n",
       "4  The National Aged Care Advocacy Program provid...   \n",
       "5  The Australia-Korea Foundation 2017 seeks to s...   \n",
       "6  The Council for Australian-Arab Relations seek...   \n",
       "7  The Australia-Japan Foundation seeks to advanc...   \n",
       "8  The Australian Cultural Diplomacy Grants Progr...   \n",
       "9  The National Women’s Alliance Grant Opportunit...   \n",
       "\n",
       "                                         Eligibility            Value  \\\n",
       "0  Any individual, organisation or business with ...              NaN   \n",
       "1  To be eligible you must: be one of the followi...  1000000-5000000   \n",
       "2  The following can apply for a grant: - Organis...              NaN   \n",
       "3  To be eligible you must: be one of the followi...  2000000-4000000   \n",
       "4  The following entity types meet the eligibilit...              NaN   \n",
       "5   To be eligible you must: be one of the follow...  1500000-3000000   \n",
       "6  To be eligible you must: be one of the followi...  2000000-4000000   \n",
       "7   To be eligible you must: be one of the follow...   500000-5000000   \n",
       "8  To be eligible you must: be one of the followi...   500000-6000000   \n",
       "9  We can only accept applications from:  existin...              NaN   \n",
       "\n",
       "                                        Location  \\\n",
       "0            ACT, NSW, VIC, SA, WA, QLD, NT, TAS   \n",
       "1  ACT, NSW, VIC, SA, WA, QLD, NT, TAS, Overseas   \n",
       "2            ACT, NSW, VIC, SA, WA, QLD, NT, TAS   \n",
       "3  ACT, NSW, VIC, SA, WA, QLD, NT, TAS, Overseas   \n",
       "4            ACT, NSW, VIC, SA, WA, QLD, NT, TAS   \n",
       "5  ACT, NSW, VIC, SA, WA, QLD, NT, TAS, Overseas   \n",
       "6  ACT, NSW, VIC, SA, WA, QLD, NT, TAS, Overseas   \n",
       "7  ACT, NSW, VIC, SA, WA, QLD, NT, TAS, Overseas   \n",
       "8  ACT, NSW, VIC, SA, WA, QLD, NT, TAS, Overseas   \n",
       "9            ACT, NSW, VIC, SA, WA, QLD, NT, TAS   \n",
       "\n",
       "                                                text  \n",
       "0  australia–china agricultural cooperation agree...  \n",
       "1  australia-asean council aac ’ mission knowledg...  \n",
       "2  -budget government year package small exporter...  \n",
       "3  council australia latin america relation coala...  \n",
       "4  national care advocacy program free independen...  \n",
       "5  australia-korea foundation seek australia-kore...  \n",
       "6  council australian-arab relation australian-ar...  \n",
       "7  australia-japan foundation australia ’ engagem...  \n",
       "8  australian cultural diplomacy grant program ac...  \n",
       "9  national woman alliance grant opportunity form...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = tenders_info.copy()[:10]\n",
    "df['text'].fillna('', inplace=True)\n",
    "assert 'text' in df.columns, f'Missing column names \"{text_col}\".'\n",
    "\n",
    "df['text'] = df['text'].map(lambda x: __convert_word_type(x))\n",
    "\n",
    "# Remove pure numbers, e.g., 2014, 20,000\n",
    "df['text'] = df['text'].map(lambda x: re.sub(r'\\s*(\\.:,|\\d+)\\s*', '', x))\n",
    "df = df.reset_index(drop=True).reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0d9a8bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cooperation agriculture australia', 0.6811), ('australia china programme', 0.6411), ('acaca agreement programme', 0.5605), ('agriculture australia china', 0.6425), ('china agricultural cooperation', 0.6282)]\n",
      "[('education australia asean', 0.6942), ('australia asean council', 0.6926), ('australia asean', 0.6915), ('knowledge understanding australia', 0.6042), ('country opportunity awareness', 0.5544)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    __extract_process(thread_cnt[0], df.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a053919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "class KeyThread(threading.Thread):\n",
    "    \n",
    "    def __init__(self,func, args=()):\n",
    "        super(MyThread,self).__init__()\n",
    "        self.func = func\n",
    "        self.args = args\n",
    "        self.result = None\n",
    "\n",
    "    def run(self):\n",
    "        self.result = self.func(*self.args)\n",
    "\n",
    "    def get_result(self):\n",
    "        if self.result:\n",
    "            return self.result\n",
    "        else:\n",
    "            self.join()\n",
    "            return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "138553e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-231:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\24966\\AppData\\Local\\Temp/ipykernel_14276/856998351.py\", line 11, in run\n",
      "  File \"C:\\Users\\24966\\AppData\\Local\\Temp/ipykernel_14276/324543592.py\", line 4, in __extract_process\n",
      "  File \"C:\\Users\\24966\\AppData\\Local\\Temp/ipykernel_14276/1638120703.py\", line 2, in __get_tags\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keybert\\_model.py\", line 113, in extract_keywords\n",
      "    keywords = self._extract_keywords_single_doc(doc=docs,\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keybert\\_model.py\", line 182, in _extract_keywords_single_doc\n",
      "    doc_embedding = self.model.embed([doc])\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keybert\\backend\\_sentencetransformers.py\", line 53, in embed\n",
      "    embeddings = self.embedding_model.encode(documents, show_progress_bar=verbose)\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\", line 160, in encode\n",
      "    features = self.tokenize(sentences_batch)\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\", line 318, in tokenize\n",
      "    return self._first_module().tokenize(texts)\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\", line 113, in tokenize\n",
      "    output.update(self.tokenizer(*to_tokenize, padding=True, truncation='longest_first', return_tensors=\"pt\", max_length=self.max_seq_length))\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2443, in __call__\n",
      "    return self.batch_encode_plus(\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2634, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\", line 416, in _batch_encode_plus\n",
      "    self.set_truncation_and_padding(\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\", line 372, in set_truncation_and_padding\n",
      "    self._tokenizer.enable_truncation(**target)\n",
      "RuntimeError: Already borrowed\n",
      "Exception in thread Thread-232:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\24966\\AppData\\Local\\Temp/ipykernel_14276/856998351.py\", line 11, in run\n",
      "  File \"C:\\Users\\24966\\AppData\\Local\\Temp/ipykernel_14276/324543592.py\", line 4, in __extract_process\n",
      "  File \"C:\\Users\\24966\\AppData\\Local\\Temp/ipykernel_14276/1638120703.py\", line 2, in __get_tags\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keybert\\_model.py\", line 113, in extract_keywords\n",
      "    keywords = self._extract_keywords_single_doc(doc=docs,\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keybert\\_model.py\", line 183, in _extract_keywords_single_doc\n",
      "    candidate_embeddings = self.model.embed(candidates)\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keybert\\backend\\_sentencetransformers.py\", line 53, in embed\n",
      "    embeddings = self.embedding_model.encode(documents, show_progress_bar=verbose)\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\", line 160, in encode\n",
      "    features = self.tokenize(sentences_batch)\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\", line 318, in tokenize\n",
      "    return self._first_module().tokenize(texts)\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\", line 113, in tokenize\n",
      "    output.update(self.tokenizer(*to_tokenize, padding=True, truncation='longest_first', return_tensors=\"pt\", max_length=self.max_seq_length))\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2443, in __call__\n",
      "    return self.batch_encode_plus(\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2634, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\", line 416, in _batch_encode_plus\n",
      "    self.set_truncation_and_padding(\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\", line 372, in set_truncation_and_padding\n",
      "    self._tokenizer.enable_truncation(**target)\n",
      "RuntimeError: Already borrowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('education australia asean', 0.6942), ('australia asean council', 0.6926), ('australia asean', 0.6915), ('knowledge understanding australia', 0.6042), ('country opportunity awareness', 0.5544)]\n",
      "[('cooperation agriculture australia', 0.6811), ('australia china programme', 0.6411), ('acaca agreement programme', 0.5605), ('agriculture australia china', 0.6425), ('china agricultural cooperation', 0.6282)]\n",
      "[4, 5, None, None]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14276/268329453.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mthread_cnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwhile\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthread_cnt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mthreads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyThread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__extract_process\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthread_cnt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mthread_cnt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyThread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__extract_process\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthread_cnt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mthread_cnt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "thread_cnt = [0,1,2,3]\n",
    "while max(thread_cnt) < len(df):\n",
    "    threads = []\n",
    "    t1 = KeyThread(__extract_process, args=(thread_cnt[0], df.iloc[thread_cnt[0]]))\n",
    "    t2 = KeyThread(__extract_process, args=(thread_cnt[1], df.iloc[thread_cnt[1]]))\n",
    "    t3 = KeyThread(__extract_process, args=(thread_cnt[2], df.iloc[thread_cnt[2]]))\n",
    "    t4 = KeyThread(__extract_process, args=(thread_cnt[3], df.iloc[thread_cnt[3]]))\n",
    "    threads.append(t1)\n",
    "    threads.append(t2)\n",
    "    threads.append(t3)\n",
    "    threads.append(t4)\n",
    "\n",
    "    for t in threads:\n",
    "        t.setDaemon(True)\n",
    "        t.start()\n",
    "        \n",
    "    for i in range(4):\n",
    "        threads[i].join()\n",
    "        thread_cnt[i] = threads[i].get_result()\n",
    "    print(thread_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee0fe09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c88562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-36:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\24966\\AppData\\Local\\Temp/ipykernel_14276/2673266896.py\", line 10, in run\n",
      "  File \"C:\\Users\\24966\\AppData\\Local\\Temp/ipykernel_14276/324543592.py\", line 4, in __extract_process\n",
      "  File \"C:\\Users\\24966\\AppData\\Local\\Temp/ipykernel_14276/1638120703.py\", line 2, in __get_tags\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keybert\\_model.py\", line 113, in extract_keywords\n",
      "    keywords = self._extract_keywords_single_doc(doc=docs,\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keybert\\_model.py\", line 182, in _extract_keywords_single_doc\n",
      "    doc_embedding = self.model.embed([doc])\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keybert\\backend\\_sentencetransformers.py\", line 53, in embed\n",
      "    embeddings = self.embedding_model.encode(documents, show_progress_bar=verbose)\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\", line 160, in encode\n",
      "    features = self.tokenize(sentences_batch)\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\", line 318, in tokenize\n",
      "    return self._first_module().tokenize(texts)\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\", line 113, in tokenize\n",
      "    output.update(self.tokenizer(*to_tokenize, padding=True, truncation='longest_first', return_tensors=\"pt\", max_length=self.max_seq_length))\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2443, in __call__\n",
      "    return self.batch_encode_plus(\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2634, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\", line 416, in _batch_encode_plus\n",
      "    self.set_truncation_and_padding(\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\", line 372, in set_truncation_and_padding\n",
      "    self._tokenizer.enable_truncation(**target)\n",
      "RuntimeError: Already borrowed\n",
      "Exception in thread Thread-37:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\24966\\AppData\\Local\\Temp/ipykernel_14276/2673266896.py\", line 10, in run\n",
      "  File \"C:\\Users\\24966\\AppData\\Local\\Temp/ipykernel_14276/324543592.py\", line 4, in __extract_process\n",
      "  File \"C:\\Users\\24966\\AppData\\Local\\Temp/ipykernel_14276/1638120703.py\", line 2, in __get_tags\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keybert\\_model.py\", line 113, in extract_keywords\n",
      "    keywords = self._extract_keywords_single_doc(doc=docs,\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keybert\\_model.py\", line 183, in _extract_keywords_single_doc\n",
      "    candidate_embeddings = self.model.embed(candidates)\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keybert\\backend\\_sentencetransformers.py\", line 53, in embed\n",
      "    embeddings = self.embedding_model.encode(documents, show_progress_bar=verbose)\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\", line 160, in encode\n",
      "    features = self.tokenize(sentences_batch)\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\", line 318, in tokenize\n",
      "    return self._first_module().tokenize(texts)\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\", line 113, in tokenize\n",
      "    output.update(self.tokenizer(*to_tokenize, padding=True, truncation='longest_first', return_tensors=\"pt\", max_length=self.max_seq_length))\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2443, in __call__\n",
      "    return self.batch_encode_plus(\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2634, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\", line 416, in _batch_encode_plus\n",
      "    self.set_truncation_and_padding(\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\", line 372, in set_truncation_and_padding\n",
      "    self._tokenizer.enable_truncation(**target)\n",
      "RuntimeError: Already borrowed\n",
      "Exception in thread Thread-35:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\24966\\AppData\\Local\\Temp/ipykernel_14276/2673266896.py\", line 10, in run\n",
      "  File \"C:\\Users\\24966\\AppData\\Local\\Temp/ipykernel_14276/324543592.py\", line 4, in __extract_process\n",
      "  File \"C:\\Users\\24966\\AppData\\Local\\Temp/ipykernel_14276/1638120703.py\", line 2, in __get_tags\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keybert\\_model.py\", line 113, in extract_keywords\n",
      "    keywords = self._extract_keywords_single_doc(doc=docs,\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keybert\\_model.py\", line 183, in _extract_keywords_single_doc\n",
      "    candidate_embeddings = self.model.embed(candidates)\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keybert\\backend\\_sentencetransformers.py\", line 53, in embed\n",
      "    embeddings = self.embedding_model.encode(documents, show_progress_bar=verbose)\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\", line 160, in encode\n",
      "    features = self.tokenize(sentences_batch)\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\", line 318, in tokenize\n",
      "    return self._first_module().tokenize(texts)\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\", line 113, in tokenize\n",
      "    output.update(self.tokenizer(*to_tokenize, padding=True, truncation='longest_first', return_tensors=\"pt\", max_length=self.max_seq_length))\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2443, in __call__\n",
      "    return self.batch_encode_plus(\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2634, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\", line 416, in _batch_encode_plus\n",
      "    self.set_truncation_and_padding(\n",
      "  File \"c:\\users\\24966\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\", line 372, in set_truncation_and_padding\n",
      "    self._tokenizer.enable_truncation(**target)\n",
      "RuntimeError: Already borrowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cooperation agriculture australia', 0.6811), ('australia china programme', 0.6411), ('acaca agreement programme', 0.5605), ('agriculture australia china', 0.6425), ('china agricultural cooperation', 0.6282)]\n",
      "[4, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "thread_cnt = [0,1,2,3]\n",
    "threads = []\n",
    "t1 = MyThread(__extract_process, args=(thread_cnt[0], df.iloc[thread_cnt[0]]))\n",
    "t2 = MyThread(__extract_process, args=(thread_cnt[1], df.iloc[thread_cnt[1]]))\n",
    "t3 = MyThread(__extract_process, args=(thread_cnt[2], df.iloc[thread_cnt[2]]))\n",
    "t4 = MyThread(__extract_process, args=(thread_cnt[3], df.iloc[thread_cnt[3]]))\n",
    "threads.append(t1)\n",
    "threads.append(t2)\n",
    "threads.append(t3)\n",
    "threads.append(t4)\n",
    "\n",
    "for t in threads:\n",
    "    t.setDaemon(True)  # 设置为守护线程，不会因主线程结束而中断\n",
    "    t.start()\n",
    "        \n",
    "for i in range(4):\n",
    "    threads[i].join()\n",
    "    thread_cnt[i] = threads[i].get_result()\n",
    "print(thread_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b0aacf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7184bfbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_pool"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
